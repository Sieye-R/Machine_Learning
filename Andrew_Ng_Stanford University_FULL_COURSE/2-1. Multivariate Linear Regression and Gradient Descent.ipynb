{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1889b96",
   "metadata": {},
   "source": [
    "# 2-1. Multivariate Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80df26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d771d39",
   "metadata": {},
   "source": [
    "### 1. Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84e7ef",
   "metadata": {},
   "source": [
    "The file Data/housing_prices.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.\n",
    "\n",
    "We load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e7ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(os.path.join('Data', 'housing_prices.txt'), delimiter=',')\n",
    "Org_X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf15be",
   "metadata": {},
   "source": [
    "The following is the first 10 rows of the data. The symbols which are used in string format are explained in Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6aeeb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         The size of the house        The number of bedrooms           The price\n",
      "\n",
      "                2104                             3                        399900\n",
      "                1600                             3                        329900\n",
      "                2400                             3                        369000\n",
      "                1416                             2                        232000\n",
      "                3000                             4                        539900\n",
      "                1985                             4                        299900\n",
      "                1534                             3                        314900\n",
      "                1427                             3                        198999\n",
      "                1380                             3                        212000\n",
      "                1494                             3                        242500\n"
     ]
    }
   ],
   "source": [
    "print('{:>30s}{:>30s}{:>20s}'.format('The size of the house', 'The number of bedrooms', 'The price'))\n",
    "print('')\n",
    "for i in range(10):\n",
    "    print('{:20.0f}{:30.0f}{:30.0f}'.format(Org_X[i, 0], Org_X[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dbeb5c",
   "metadata": {},
   "source": [
    "### 2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f8cbc",
   "metadata": {},
   "source": [
    "As you can see, the ranges of first two columns are very different. So if we find the minimal point $\\theta$ which minimizes the cost function $J(\\theta)$, it would be hard to find a proper constant $\\alpha$ such that $J(\\theta)$ converges because $J(\\theta)$ does not have much convexity. (The formulas for the cost function and gradient descent can be found in Section 4 and Section 5, respectively.) To avoid from this situation, we first scale the data. We can consider standardization, min-max normalization or mean normalization. \n",
    "\n",
    "(1) If we scale the data with standardization, then the mean and the standard deviation of the resulting data would be zero and one, respectively.\n",
    "\n",
    "(2) If we scale the data with min-max normalization, then the range of the resulting data would be [0, 1].\n",
    "\n",
    "(3) If we scale the data with mean normalization, then the range of the resulting data would be [-1, 1].\n",
    "\n",
    "For details, see https://github.com/Sieye-R/Machine_Learning/blob/main/Normalization.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaec92e",
   "metadata": {},
   "source": [
    "### (1) Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e4864",
   "metadata": {},
   "source": [
    "We standardize the data. Suppose that $X$ has $2$ features $X^{(1)}$ and $X^{(2)}$ and $m$ training sets. If $\\mu^{(i)}$ and $\\sigma^{(i)}$ are the mean and the standard deviation of the feature $X^{(i)}$, respectively, then we replace $x_j^{(i)}$ with \n",
    "$$\\frac{x_j^{(i)}-\\mu^{(i)}}{\\sigma^{(i)}}$$\n",
    "for each $i = 1, 2$ and $j=1, \\cdots, m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a898d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardization(X):\n",
    "    X_stand = Org_X.copy()\n",
    "    mu = np.mean(Org_X, axis=0)\n",
    "    sigma = np.std(Org_X, axis=0)\n",
    "    X_stand = (Org_X - mu)/sigma\n",
    "    return X_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375e163",
   "metadata": {},
   "source": [
    "The following is the first 10 rows of the data after standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410b0b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         The size of the house        The number of bedrooms           The price\n",
      "\n",
      "              0.1314                       -0.2261                   399900.0000\n",
      "             -0.5096                       -0.2261                   329900.0000\n",
      "              0.5079                       -0.2261                   369000.0000\n",
      "             -0.7437                       -1.5544                   232000.0000\n",
      "              1.2711                        1.1022                   539900.0000\n",
      "             -0.0199                        1.1022                   299900.0000\n",
      "             -0.5936                       -0.2261                   314900.0000\n",
      "             -0.7297                       -0.2261                   198999.0000\n",
      "             -0.7895                       -0.2261                   212000.0000\n",
      "             -0.6445                       -0.2261                   242500.0000\n"
     ]
    }
   ],
   "source": [
    "print('{:>30s}{:>30s}{:>20s}'.format('The size of the house', 'The number of bedrooms', 'The price'))\n",
    "print('')\n",
    "for i in range(10):\n",
    "    print('{:20.4f}{:30.4f}{:30.4f}'.format(Standardization(Org_X)[i, 0], Standardization(Org_X)[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79be9d0",
   "metadata": {},
   "source": [
    "We can also standardize the data with Scikitlearn as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88928d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         The size of the house        The number of bedrooms           The price\n",
      "\n",
      "              0.1314                       -0.2261                        399900\n",
      "             -0.5096                       -0.2261                        329900\n",
      "              0.5079                       -0.2261                        369000\n",
      "             -0.7437                       -1.5544                        232000\n",
      "              1.2711                        1.1022                        539900\n",
      "             -0.0199                        1.1022                        299900\n",
      "             -0.5936                       -0.2261                        314900\n",
      "             -0.7297                       -0.2261                        198999\n",
      "             -0.7895                       -0.2261                        212000\n",
      "             -0.6445                       -0.2261                        242500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Org_X)\n",
    "print('{:>30s}{:>30s}{:>20s}'.format('The size of the house', 'The number of bedrooms', 'The price'))\n",
    "print('')\n",
    "for i in range(10):\n",
    "    print('{:20.4f}{:30.4f}{:30.0f}'.format(scaler.transform(Org_X)[i, 0], scaler.transform(Org_X)[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c100ec",
   "metadata": {},
   "source": [
    "### (2) Min-Max Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e39de0",
   "metadata": {},
   "source": [
    "Suppose that $X$ has $2$ features $X^{(1)}$ and $X^{(2)}$ and $m$ training sets. If $m^{(i)}$ and $M^{(i)}$ are the minimum and the maximum of the feature $X^{(i)}$, respectively, then we replace $x_j^{(i)}$ with \n",
    "$$\\frac{x_j^{(i)}-m^{(i)}}{M^{(i)}-m^{(i)}}$$\n",
    "for each $i = 1, 2$ and $j=1, \\cdots, m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c389d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Min_Max(X):\n",
    "    Max = np.max(Org_X, axis=0)\n",
    "    Min = np.min(Org_X, axis=0)\n",
    "    X_mm = (Org_X - Min)/(Max-Min)\n",
    "    return X_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44a119",
   "metadata": {},
   "source": [
    "The following is the first 10 rows after min_max normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66347c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         The size of the house        The number of bedrooms           The price\n",
      "\n",
      "              0.3453                        0.5000                        399900\n",
      "              0.2063                        0.5000                        329900\n",
      "              0.4269                        0.5000                        369000\n",
      "              0.1555                        0.2500                        232000\n",
      "              0.5924                        0.7500                        539900\n",
      "              0.3125                        0.7500                        299900\n",
      "              0.1881                        0.5000                        314900\n",
      "              0.1586                        0.5000                        198999\n",
      "              0.1456                        0.5000                        212000\n",
      "              0.1771                        0.5000                        242500\n"
     ]
    }
   ],
   "source": [
    "print('{:>30s}{:>30s}{:>20s}'.format('The size of the house', 'The number of bedrooms', 'The price'))\n",
    "print('')\n",
    "for i in range(10):\n",
    "    print('{:20.4f}{:30.4f}{:30.0f}'.format(Min_Max(Org_X)[i, 0], Min_Max(Org_X)[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739afdaa",
   "metadata": {},
   "source": [
    "### (3) Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cdf4bf",
   "metadata": {},
   "source": [
    "Suppose that $X$ has $2$ features $X^{(1)}$ and $X^{(2)}$ and $m$ training sets. If $\\mu^{(i)}$, $m^{(i)}$ and $M^{(i)}$ are the mean, minimum and the maximum of the feature $X^{(i)}$, respectively, then we replace $x_j^{(i)}$ with \n",
    "$$\\frac{x_j^{(i)}-\\mu^{(i)}}{M^{(i)}-m^{(i)}}$$\n",
    "for each $i = 1, 2$ and $j=1, \\cdots, m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec775e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanNormalization(X):\n",
    "    X_MeanNorm = Org_X.copy()\n",
    "    mu = np.mean(Org_X, axis=0)\n",
    "    Max = np.max(Org_X, axis=0)\n",
    "    Min = np.min(Org_X, axis=0)\n",
    "    X_MeanNorm = (Org_X - mu)/(Max-Min)\n",
    "    return X_MeanNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69406b05",
   "metadata": {},
   "source": [
    "The following is the first 10 rows after mean normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b322ba0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         The size of the house        The number of bedrooms           The price\n",
      "\n",
      "              0.0285                       -0.0426                        399900\n",
      "             -0.1105                       -0.0426                        329900\n",
      "              0.1101                       -0.0426                        369000\n",
      "             -0.1612                       -0.2926                        232000\n",
      "              0.2756                        0.2074                        539900\n",
      "             -0.0043                        0.2074                        299900\n",
      "             -0.1287                       -0.0426                        314900\n",
      "             -0.1582                       -0.0426                        198999\n",
      "             -0.1712                       -0.0426                        212000\n",
      "             -0.1397                       -0.0426                        242500\n"
     ]
    }
   ],
   "source": [
    "print('{:>30s}{:>30s}{:>20s}'.format('The size of the house', 'The number of bedrooms', 'The price'))\n",
    "print('')\n",
    "for i in range(10):\n",
    "    print('{:20.4f}{:30.4f}{:30.0f}'.format(MeanNormalization(Org_X)[i, 0], MeanNormalization(Org_X)[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c73499",
   "metadata": {},
   "source": [
    "In this notebook, we will normalize the data by mean normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67857b1a",
   "metadata": {},
   "source": [
    "### 3. Modification of x (add a column of 1's for the intercept term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837580d",
   "metadata": {},
   "source": [
    "Suppose that we have $n$ features $X^{(1)}, \\cdots, X^{(n)}$ and each feature $X^{(j)}$ has $m$ training datasets\n",
    "consisting of independent variables $x_1^{(j)}, \\cdots, x_m^{(j)}$ and that dependent variables are $y_1, \\cdots, y_m$.\n",
    "Then the linear regression predictor of the data is of the form\n",
    "$$y = \\theta_0 + \\theta_1 x^{(1)} + \\theta_2 x^{(2)} + \\cdots + \\theta_n x^{(n)}.$$ \n",
    "\n",
    "We define the $m \\times (n+1)$ matrix $X$ by\n",
    "$$X(i, 1) = 1 \\qquad X(i, j+1) = x_i^{(j)} \\qquad (i = 1, \\cdots, m, \\; j=1, \\cdots n).$$\n",
    "We also define the column vectors $y \\in \\mathbb{R}^m$ and $\\theta \\in \\mathbb{R}^{n+1}$ by\n",
    "$$y(i) = y_i \\qquad (1 \\leq i \\leq m)$$\n",
    "and\n",
    "$$\\theta(i) = \\theta_{i-1} \\qquad (i=1, \\cdots, n+1).$$\n",
    "\n",
    "Then the formula of the least square is given by\n",
    "$$\\sum_{i=1}^m \\, \\{(X \\theta-y)(i)\\}^2.$$\n",
    "\n",
    "In the following cell, we add a column of 1's and construct an $m \\times (n+1)$ matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc2e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([np.ones((m, 1)), MeanNormalization(Org_X)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a80847",
   "metadata": {},
   "source": [
    "### 4. Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b74c46a",
   "metadata": {},
   "source": [
    "The cost function $J(\\theta_0, \\theta_1, \\cdots, \\theta_n)$ is defined by\n",
    "$$J(\\theta_0, \\theta_1, \\cdots, \\theta_n) = \\frac{1}{2m} \\sum_{i=1}^m \\, \\{(X \\theta-y)(i)\\}^2.$$\n",
    "This value is the mean of least squares devided by $2$.\n",
    "When we partial differentiate $J$, $2$ is eliminated by the chain rule.\n",
    "Except this, $2$ does not play any role when we partial differentiate $J$.\n",
    "\n",
    "We implement the cost function $J(\\theta_0, \\theta_1, \\cdots, \\theta_n)$ with Python. Here, we confusingly use $\\theta$ whose shape is (3, ) in our Python language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "050cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostMulti(X, y, theta):\n",
    "    m = y.shape[0] \n",
    "    J = 0\n",
    "    dot_prod=np.dot(X, theta)\n",
    "    J = np.dot(dot_prod-y, dot_prod-y)\n",
    "    J /= 2*m\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5705e46",
   "metadata": {},
   "source": [
    "The following is the cost of $(\\theta_0, \\theta_1, \\theta_2)=(0,0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50bf13ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J(0, 0) = 65591548106.46\n"
     ]
    }
   ],
   "source": [
    "J = computeCostMulti(X, y, theta=np.array([0.0, 0.0, 0.0]))\n",
    "print('J(0, 0) = %.2f' % J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a26b9",
   "metadata": {},
   "source": [
    "### 5. Gradient Descent\n",
    "\n",
    "\n",
    "We set\n",
    "$$\\theta_j^{(0)} = \\theta_j \\qquad (j = 0, 1, 2)$$\n",
    "and\n",
    "$$\\theta_j^{(r)} = \\theta_j^{(r-1)} - \\alpha \\frac{\\partial J}{\\partial \\theta_j}(\\theta_0^{(r-1)}, \\theta_1^{(r-1)}, \\theta_2^{(r-1)}) \\qquad (r=1, 2, \\cdots).$$\n",
    "For sutible $\\alpha$, \n",
    "the sequence $g(\\alpha) = \\{(\\theta_0^{(r)}, \\theta_1^{(r)}, \\theta_2^{(r)})\\}$ converges to the minimum point of $J(\\theta_0, \\theta_1, \\theta_2)$ as $r$ goes to the infinity. \n",
    "\n",
    "The direct computation shows that\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j}(\\theta_0, \\theta_1, \\theta_2) = \\frac{1}{m} \\sum_{i=1}^m \\, (X\\theta - y)(i)X_i^{(j)} \\qquad (j=0, 1, 2)$$\n",
    "\n",
    "We implement gradient descent with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0045c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, iterations):\n",
    "    m = y.shape[0] \n",
    "    theta = theta.copy()\n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        dotprod = np.dot(X, theta)\n",
    "        theta = theta - (alpha / m) * np.dot(dotprod-y, X)\n",
    "        J_history.append(computeCostMulti(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88005c89",
   "metadata": {},
   "source": [
    "The following graph shows that $g(\\alpha)$ converges when $\\alpha=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "960a8176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "iterations = 1000\n",
    "\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, iterations)\n",
    "\n",
    "plt.plot(np.arange(len(J_history)), J_history, lw=2) #lw = linewidth\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a9d54",
   "metadata": {},
   "source": [
    "The outputs of the following two cells show that what the minimum point $\\theta$ of $J$ is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc46289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[340412.65957447 504777.89981689 -34952.0710886 ]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "iterations = 2000\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, iterations)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef34ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[340412.65957447 504777.90398732 -34952.07644855]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "iterations = 3000\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, iterations)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee61c3a",
   "metadata": {},
   "source": [
    "### 6. Linear Regression Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ad8f740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hence, the linear regression predictor is y = 340412.6596 + 504777.9040 x^(1) + -34952.0764 x^(2).\n"
     ]
    }
   ],
   "source": [
    "print('Hence, the linear regression predictor is y = {:.4f} + {:.4f} x^(1) + {:.4f} x^(2).'.format(*theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1a882",
   "metadata": {},
   "source": [
    "Now, we predict the price of the house whose size is 1650 square feet with 3 bedrooms. \n",
    "We need to apply for the same normalization to $[1650, 3]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b779c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "house = np.array([1650, 3])\n",
    "mu = np.mean(Org_X, axis=0)\n",
    "Max = np.max(Org_X, axis=0)\n",
    "Min = np.min(Org_X, axis=0)\n",
    "house_mean_normalization = (house - mu)/(Max-Min)\n",
    "house_final = X = np.concatenate([np.ones(1), house_mean_normalization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a030864",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650 square feet and three bedrooms house \n",
      " the predicted price by gradient descent is $293081\n"
     ]
    }
   ],
   "source": [
    "predict = house_final @ theta\n",
    "print('1650 square feet and three bedrooms house \\n the predicted price by gradient descent is ${:.0f}'.format(predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8b671",
   "metadata": {},
   "source": [
    "### 7. Gradient Descent without Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36dfb11",
   "metadata": {},
   "source": [
    "In this section, we plot the graph of $J(\\theta)$ when $\\alpha=0.001$ without normalization. We can compare the result with the 15-th cell. We chose 0.5 for $\\alpha$ in the 15-th cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd0a930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([np.ones((m, 1)), Org_X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "139dcef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "iterations = 50\n",
    "\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, iterations)\n",
    "\n",
    "plt.plot(np.arange(len(J_history)), J_history, lw=2) #lw = linewidth\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b7c67",
   "metadata": {},
   "source": [
    "### 8. Normal Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50989cca",
   "metadata": {},
   "source": [
    "In Section 3 of <a href=\"https://github.com/Sieye-R/Machine_Learning/blob/main/Linear%20Regression%20and%20Polynomial%20Regression.pdf\">Linear Regression and Polynomial Regression.pdf</a>, \n",
    "it is proved that $\\theta$ can be expressed in terms of matrices $X$ and $y$ as follows:\n",
    "$$\\theta = (X^{\\textsf{T}}X)^{-1}X^{\\textsf{T}}y.$$\n",
    "When we directly calculate the minimal point of the cost function $J(\\theta)$, we do not need to normaliza the data. The cost function is convex so it has the minimum. How convex the graph of $J$ is does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20ae6178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEqn(X, y):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for1 = np.dot(X.T, X)\n",
    "    for2 = np.linalg.inv(for1)\n",
    "    for3 = np.dot(X.T, y)\n",
    "    theta = np.dot(for2, for3)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9411aa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed from the normal equations: [89597.9095428    139.21067402 -8738.01911233]\n"
     ]
    }
   ],
   "source": [
    "theta = normalEqn(X, y)\n",
    "print('Theta computed from the normal equations: {:s}'.format(str(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94a636da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([89597.9095428 ,   139.21067402, -8738.01911233])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53fcf8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650 square feet and three bedrooms house \n",
      " the predicted price by gradient descent is $293081\n"
     ]
    }
   ],
   "source": [
    "X_array = [1, 1650, 3]\n",
    "price = np.dot(X_array, theta)\n",
    "print('1650 square feet and three bedrooms house \\n the predicted price by gradient descent is ${:.0f}'.format(price))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
